<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Muthukrishnan">
<meta name="dcterms.date" content="2025-02-09">
<meta name="description" content="Scrapy is a powerful and highly versatile open-source web scraping framework written in Python. It’s designed to efficiently extract data from websites,…">

<title>scrapy - Documentation – Technical Manuals</title>
<style>
html {
  color: #1a1a1a;
  background-color: #fdfdfd;
}
body {
  margin: 0 auto;
  max-width: 36em;
  padding-left: 50px;
  padding-right: 50px;
  padding-top: 50px;
  padding-bottom: 50px;
  hyphens: auto;
  overflow-wrap: break-word;
  text-rendering: optimizeLegibility;
  font-kerning: normal;
}
@media (max-width: 600px) {
  body {
    font-size: 0.9em;
    padding: 12px;
  }
  h1 {
    font-size: 1.8em;
  }
}
@media print {
  html {
    background-color: white;
  }
  body {
    background-color: transparent;
    color: black;
    font-size: 12pt;
  }
  p, h2, h3 {
    orphans: 3;
    widows: 3;
  }
  h2, h3, h4 {
    page-break-after: avoid;
  }
}
p {
  margin: 1em 0;
}
a {
  color: #1a1a1a;
}
a:visited {
  color: #1a1a1a;
}
img {
  max-width: 100%;
}
svg {
  height; auto;
  max-width: 100%;
}
h1, h2, h3, h4, h5, h6 {
  margin-top: 1.4em;
}
h5, h6 {
  font-size: 1em;
  font-style: italic;
}
h6 {
  font-weight: normal;
}
ol, ul {
  padding-left: 1.7em;
  margin-top: 1em;
}
li > ol, li > ul {
  margin-top: 0;
}
ul > li:not(:has(> p)) > ul,
ol > li:not(:has(> p)) > ul,
ul > li:not(:has(> p)) > ol,
ol > li:not(:has(> p)) > ol {
  margin-bottom: 0;
}
ul > li:not(:has(> p)) > ul > li:has(> p),
ol > li:not(:has(> p)) > ul > li:has(> p),
ul > li:not(:has(> p)) > ol > li:has(> p),
ol > li:not(:has(> p)) > ol > li:has(> p) {
  margin-top: 1rem;
}
blockquote {
  margin: 1em 0 1em 1.7em;
  padding-left: 1em;
  border-left: 2px solid #e6e6e6;
  color: #606060;
}
code {
  font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
  font-size: 85%;
  margin: 0;
  hyphens: manual;
}
pre {
  margin: 1em 0;
  overflow: auto;
}
pre code {
  padding: 0;
  overflow: visible;
  overflow-wrap: normal;
}
.sourceCode {
 background-color: transparent;
 overflow: visible;
}
hr {
  border: none;
  border-top: 1px solid #1a1a1a;
  height: 1px;
  margin: 1em 0;
}
table {
  margin: 1em 0;
  border-collapse: collapse;
  width: 100%;
  overflow-x: auto;
  display: block;
  font-variant-numeric: lining-nums tabular-nums;
}
table caption {
  margin-bottom: 0.75em;
}
tbody {
  margin-top: 0.5em;
  border-top: 1px solid #1a1a1a;
  border-bottom: 1px solid #1a1a1a;
}
th {
  border-top: 1px solid #1a1a1a;
  padding: 0.25em 0.5em 0.25em 0.5em;
}
td {
  padding: 0.125em 0.5em 0.25em 0.5em;
}
header {
  margin-bottom: 4em;
  text-align: center;
}
#TOC li {
  list-style: none;
}
#TOC ul {
  padding-left: 1.3em;
}
#TOC > ul {
  padding-left: 0;
}
#TOC a:not(:hover) {
  text-decoration: none;
}
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<link href="../../favicon.ico" rel="icon">
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-MXDPF6L5TL"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-MXDPF6L5TL', { 'anonymize_ip': true});
</script>
<link rel="icon" type="image/x-icon" href="favicon.ico">
<meta property="og:title" content="scrapy - Documentation – Technical Manuals">
<meta property="og:description" content="Comprehensive technical documentation and tutorials for JavaScript libraries and Python modules.">
<meta property="og:image" content="https://manuals.muthu.co//og-image.png">
<meta property="og:site_name" content="Technical Manuals">
<meta property="og:locale" content="en_US">
<meta property="og:image:height" content="630">
<meta property="og:image:width" content="1200">
<meta name="twitter:title" content="scrapy - Documentation – Technical Manuals">
<meta name="twitter:description" content="Comprehensive technical documentation and tutorials for JavaScript libraries and Python modules.">
<meta name="twitter:image" content="https://manuals.muthu.co//og-image.png">
<meta name="twitter:creator" content="@krimuthu">
<meta name="twitter:site" content="@krimuthu">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image-height" content="630">
<meta name="twitter:image-width" content="1200">
<link rel="canonical" href="https://manuals.muthu.co/posts/python-modules/scrapy.html">
</head><body><div class="navigation-header">
    <nav>
        <div>
            <div class="logo">
                <a href="../../" aria-label="Home">
                    <span>Technical Manuals - Home</span>
                </a>
            </div>
            <div class="nav-menu">
                <ul>
                    <li>
                        <a href="../../about.html">
                            <span class="menu-text">About</span>
                        </a>
                    </li>
                    <li> 
                        <a href="https://github.com/muthuspark" target="_blank">
                            <span class="menu-text">Github</span>
                        </a>
                    </li>
                    <li>
                        <a href="https://linkedin.com/in/krimuthu" target="_blank">
                            <span class="menu-text">Linkedin</span>
                        </a>
                    </li>
                    <li>
                        <button onclick="window.print()" class="print-button">
                            <svg width="20" height="20" viewbox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                              <path d="M6 9V2h12v7"></path>
                              <path d="M6 18H4a2 2 0 01-2-2v-5a2 2 0 012-2h16a2 2 0 012 2v5a2 2 0 01-2 2h-2"></path>
                              <path d="M6 14h12v8H6z"></path>
                            </svg>
                            Print Page
                        </button>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
</div>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebSite",
  "name": "Technical Manuals",
  "url": "https://manuals.muthu.co/",
  "description": "Comprehensive technical documentation and tutorials for JavaScript libraries and Python modules.",
  "author": {
    "@type": "Person",
    "name": "Muthukrishnan",
    "url": "https://manuals.muthu.co/about.html",
    "sameAs": [
      "https://github.com/muthuspark",
      "https://linkedin.com/in/krimuthu",
      "https://twitter.com/krimuthu"
    ]
  },
  "publisher": {
    "@type": "Person",
    "name": "Muthukrishnan"
  },
  "inLanguage": "en-US",
  "potentialAction": {
    "@type": "SearchAction",
    "target": "https://manuals.muthu.co/?q={search_term_string}",
    "query-input": "required name=search_term_string"
  }
}
</script>
<meta name="author" content="Muthukrishnan">
<meta name="robots" content="index, follow">
<meta name="googlebot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
<link rel="author" href="https://manuals.muthu.co/about.html">


<link rel="stylesheet" href="../../styles.css">





<header id="title-block-header">
<h1 class="title">scrapy - Documentation</h1>

<p class="date">2025-02-09</p>
</header>

<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-scrapy" id="toc-what-is-scrapy">What is Scrapy?</a></li>
  <li><a href="#why-use-scrapy" id="toc-why-use-scrapy">Why use Scrapy?</a></li>
  <li><a href="#setting-up-scrapy" id="toc-setting-up-scrapy">Setting up Scrapy</a></li>
  <li><a href="#project-structure" id="toc-project-structure">Project Structure</a></li>
  <li><a href="#selectors" id="toc-selectors">Selectors</a>
  <ul>
  <li><a href="#xpath-selectors" id="toc-xpath-selectors">XPath Selectors</a></li>
  <li><a href="#css-selectors" id="toc-css-selectors">CSS Selectors</a></li>
  <li><a href="#extracting-data-with-selectors" id="toc-extracting-data-with-selectors">Extracting Data with Selectors</a></li>
  <li><a href="#selector-lists-and-iteration" id="toc-selector-lists-and-iteration">Selector Lists and Iteration</a></li>
  </ul></li>
  <li><a href="#spiders" id="toc-spiders">Spiders</a>
  <ul>
  <li><a href="#creating-a-spider" id="toc-creating-a-spider">Creating a Spider</a></li>
  <li><a href="#spider-attributes" id="toc-spider-attributes">Spider Attributes</a></li>
  <li><a href="#requesting-urls" id="toc-requesting-urls">Requesting URLs</a></li>
  <li><a href="#parsing-responses" id="toc-parsing-responses">Parsing Responses</a></li>
  <li><a href="#handling-different-response-types" id="toc-handling-different-response-types">Handling Different Response Types</a></li>
  <li><a href="#spider-middleware" id="toc-spider-middleware">Spider Middleware</a></li>
  <li><a href="#rules-and-crawlspider" id="toc-rules-and-crawlspider">Rules and CrawlSpider</a></li>
  </ul></li>
  <li><a href="#items" id="toc-items">Items</a>
  <ul>
  <li><a href="#defining-items" id="toc-defining-items">Defining Items</a></li>
  <li><a href="#working-with-item-loaders" id="toc-working-with-item-loaders">Working with Item Loaders</a></li>
  <li><a href="#item-pipelines" id="toc-item-pipelines">Item Pipelines</a></li>
  </ul></li>
  <li><a href="#item-pipelines-1" id="toc-item-pipelines-1">Item Pipelines</a>
  <ul>
  <li><a href="#pipeline-structure" id="toc-pipeline-structure">Pipeline Structure</a></li>
  <li><a href="#common-pipeline-operations" id="toc-common-pipeline-operations">Common Pipeline Operations</a></li>
  <li><a href="#custom-pipelines" id="toc-custom-pipelines">Custom Pipelines</a></li>
  <li><a href="#pipeline-ordering" id="toc-pipeline-ordering">Pipeline Ordering</a></li>
  </ul></li>
  <li><a href="#data-storage" id="toc-data-storage">Data Storage</a>
  <ul>
  <li><a href="#storing-data-in-files" id="toc-storing-data-in-files">Storing Data in Files</a></li>
  <li><a href="#storing-data-in-databases" id="toc-storing-data-in-databases">Storing Data in Databases</a></li>
  <li><a href="#exporting-data-in-various-formats" id="toc-exporting-data-in-various-formats">Exporting Data in Various Formats</a></li>
  </ul></li>
  <li><a href="#request-and-response-handling" id="toc-request-and-response-handling">Request and Response Handling</a>
  <ul>
  <li><a href="#making-requests" id="toc-making-requests">Making Requests</a></li>
  <li><a href="#handling-responses" id="toc-handling-responses">Handling Responses</a></li>
  <li><a href="#http-headers" id="toc-http-headers">HTTP Headers</a></li>
  <li><a href="#cookies" id="toc-cookies">Cookies</a></li>
  <li><a href="#request-and-response-middleware" id="toc-request-and-response-middleware">Request and Response Middleware</a></li>
  </ul></li>
  <li><a href="#middleware" id="toc-middleware">Middleware</a>
  <ul>
  <li><a href="#downloader-middleware" id="toc-downloader-middleware">Downloader Middleware</a></li>
  <li><a href="#spider-middleware-1" id="toc-spider-middleware-1">Spider Middleware</a></li>
  <li><a href="#creating-custom-middleware" id="toc-creating-custom-middleware">Creating Custom Middleware</a></li>
  </ul></li>
  <li><a href="#settings" id="toc-settings">Settings</a>
  <ul>
  <li><a href="#configuring-scrapy" id="toc-configuring-scrapy">Configuring Scrapy</a></li>
  <li><a href="#setting-file" id="toc-setting-file">Setting File</a></li>
  <li><a href="#command-line-options" id="toc-command-line-options">Command-Line Options</a></li>
  <li><a href="#environment-variables" id="toc-environment-variables">Environment Variables</a></li>
  </ul></li>
  <li><a href="#crawling-strategies" id="toc-crawling-strategies">Crawling Strategies</a>
  <ul>
  <li><a href="#depth-first-crawling" id="toc-depth-first-crawling">Depth-First Crawling</a></li>
  <li><a href="#breadth-first-crawling" id="toc-breadth-first-crawling">Breadth-First Crawling</a></li>
  <li><a href="#scheduling-requests" id="toc-scheduling-requests">Scheduling Requests</a></li>
  <li><a href="#prioritizing-requests" id="toc-prioritizing-requests">Prioritizing Requests</a></li>
  </ul></li>
  <li><a href="#debugging-and-logging" id="toc-debugging-and-logging">Debugging and Logging</a>
  <ul>
  <li><a href="#debugging-scrapy-applications" id="toc-debugging-scrapy-applications">Debugging Scrapy Applications</a></li>
  <li><a href="#logging-in-scrapy" id="toc-logging-in-scrapy">Logging in Scrapy</a></li>
  <li><a href="#debugging-tools" id="toc-debugging-tools">Debugging Tools</a></li>
  </ul></li>
  <li><a href="#testing" id="toc-testing">Testing</a>
  <ul>
  <li><a href="#unit-testing-scrapy-spiders" id="toc-unit-testing-scrapy-spiders">Unit Testing Scrapy Spiders</a></li>
  <li><a href="#integration-testing" id="toc-integration-testing">Integration Testing</a></li>
  <li><a href="#testing-pipelines" id="toc-testing-pipelines">Testing Pipelines</a></li>
  </ul></li>
  <li><a href="#advanced-topics" id="toc-advanced-topics">Advanced Topics</a>
  <ul>
  <li><a href="#working-with-javascript" id="toc-working-with-javascript">Working with JavaScript</a></li>
  <li><a href="#handling-authentication" id="toc-handling-authentication">Handling Authentication</a></li>
  <li><a href="#proxies-and-ip-rotation" id="toc-proxies-and-ip-rotation">Proxies and IP Rotation</a></li>
  <li><a href="#distributed-crawling" id="toc-distributed-crawling">Distributed Crawling</a></li>
  <li><a href="#scrapy-extensions" id="toc-scrapy-extensions">Scrapy Extensions</a></li>
  </ul></li>
  <li><a href="#deploying-scrapy-projects" id="toc-deploying-scrapy-projects">Deploying Scrapy Projects</a>
  <ul>
  <li><a href="#deploying-to-a-server" id="toc-deploying-to-a-server">Deploying to a Server</a></li>
  <li><a href="#scheduling-crawls" id="toc-scheduling-crawls">Scheduling Crawls</a></li>
  <li><a href="#monitoring-performance" id="toc-monitoring-performance">Monitoring Performance</a></li>
  </ul></li>
  </ul>
</nav>
<h3 id="what-is-scrapy">What is Scrapy?</h3>
<p>Scrapy is a powerful and highly versatile open-source web scraping framework written in Python. It’s designed to efficiently extract data from websites, providing a structured and scalable approach to web scraping tasks. Beyond simple data extraction, Scrapy allows for the creation of web spiders that can crawl websites, follow links, and process the extracted data according to your specifications. This makes it suitable for a wide range of applications, from data mining and research to monitoring websites and building data pipelines. Its robust architecture promotes code reusability and maintainability, making it an ideal choice for both small and large-scale web scraping projects.</p>
<h3 id="why-use-scrapy">Why use Scrapy?</h3>
<p>Scrapy offers several compelling advantages over other web scraping methods:</p>
<ul>
<li><p><strong>High Performance:</strong> Scrapy’s asynchronous architecture enables efficient parallel requests, significantly speeding up the scraping process compared to sequential methods. It leverages features like built-in concurrency and downloader middleware to optimize performance.</p></li>
<li><p><strong>Scalability:</strong> Scrapy is designed for scalability. You can easily expand your scraping projects to handle larger websites and larger datasets by adding more resources (e.g., more machines, more spiders).</p></li>
<li><p><strong>Structured and Organized:</strong> Scrapy enforces a well-defined project structure and utilizes a component-based architecture, making it easier to organize, maintain, and debug large scraping projects.</p></li>
<li><p><strong>Extensible:</strong> Scrapy provides numerous extensions and middleware points, enabling you to customize and extend its functionality to suit specific requirements. This allows for integration with various tools and services.</p></li>
<li><p><strong>Large Community and Support:</strong> Scrapy benefits from a large and active community, providing extensive documentation, tutorials, and readily available support.</p></li>
<li><p><strong>Built-in Support for Multiple Formats:</strong> Scrapy easily handles various data formats like JSON, XML, and CSV, simplifying data processing and storage.</p></li>
<li><p><strong>Selectors:</strong> Scrapy provides powerful selectors (XPath, CSS) making it easy to extract data from HTML and XML responses efficiently and accurately.</p></li>
</ul>
<h3 id="setting-up-scrapy">Setting up Scrapy</h3>
<p>Setting up Scrapy is straightforward:</p>
<ol type="1">
<li><p><strong>Install Python:</strong> Ensure you have Python 3.7 or higher installed on your system. You can download it from <a href="https://www.python.org/">https://www.python.org/</a>.</p></li>
<li><p><strong>Install Scrapy:</strong> Open your terminal or command prompt and use pip to install Scrapy:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install scrapy</span></code></pre></div></li>
<li><p><strong>Verify Installation:</strong> After installation, verify it by running:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">scrapy</span> version</span></code></pre></div>
<p>This should display the installed Scrapy version.</p></li>
<li><p><strong>Create a Scrapy Project:</strong> Use the <code>scrapy startproject</code> command to create a new project:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">scrapy</span> startproject my_project</span></code></pre></div>
<p>Replace <code>my_project</code> with your desired project name.</p></li>
</ol>
<h3 id="project-structure">Project Structure</h3>
<p>A Scrapy project created with <code>scrapy startproject</code> will have the following directory structure:</p>
<ul>
<li><strong>myproject/ (Project Root)</strong>: The top-level directory of your project.
<ul>
<li><strong>myproject/ (Package):</strong> Contains the project’s core code.
<ul>
<li><strong><strong>init</strong>.py:</strong> An empty file marking the directory as a Python package.</li>
<li><strong>items.py:</strong> Defines the data structures (items) to store scraped data.</li>
<li><strong>middlewares.py:</strong> Contains middleware components to modify Scrapy’s request/response processing.</li>
<li><strong>pipelines.py:</strong> Defines pipelines for data processing and storage after scraping.</li>
<li><strong>settings.py:</strong> Contains project-wide settings.</li>
<li><strong>spiders/:</strong> Contains your spiders (web scraping scripts).</li>
</ul></li>
<li><strong>scrapy.cfg:</strong> Scrapy project configuration file.</li>
</ul></li>
</ul>
<p>This structure promotes organization and modularity, making it easier to manage complex scraping projects with multiple spiders and data processing components. Each component plays a crucial role in the scraping process.</p>
<h2 id="selectors">Selectors</h2>
<p>Scrapy provides powerful selectors to extract data from HTML and XML responses. These selectors allow you to target specific elements within the document using XPath and CSS expressions.</p>
<h3 id="xpath-selectors">XPath Selectors</h3>
<p>XPath is a query language for selecting nodes in XML documents. Scrapy leverages XPath’s ability to navigate and filter HTML (which is essentially a form of XML) to extract specific data points. XPath expressions are strings that specify the path to the desired elements.</p>
<p><strong>Example:</strong></p>
<p>Consider this HTML snippet:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">html</span><span class="dt">&gt;</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">body</span><span class="dt">&gt;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">h1</span><span class="dt">&gt;</span>My Title<span class="dt">&lt;/</span><span class="kw">h1</span><span class="dt">&gt;</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">p</span><span class="dt">&gt;</span>This is some text.<span class="dt">&lt;/</span><span class="kw">p</span><span class="dt">&gt;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">p</span><span class="ot"> class</span><span class="op">=</span><span class="st">"price"</span><span class="dt">&gt;</span>$100<span class="dt">&lt;/</span><span class="kw">p</span><span class="dt">&gt;</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">body</span><span class="dt">&gt;</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">html</span><span class="dt">&gt;</span></span></code></pre></div>
<p>To extract the title using XPath:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scrapy.selector <span class="im">import</span> Selector</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>html_content <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;html&gt;</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;body&gt;</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="st">  &lt;h1&gt;My Title&lt;/h1&gt;</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="st">  &lt;p&gt;This is some text.&lt;/p&gt;</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="st">  &lt;div&gt;</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="st">    &lt;p class="price"&gt;$100&lt;/p&gt;</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="st">  &lt;/div&gt;</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/body&gt;</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/html&gt;</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>selector <span class="op">=</span> Selector(text<span class="op">=</span>html_content)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> selector.xpath(<span class="st">"//h1/text()"</span>).get()  <span class="co"># //h1 selects all h1 elements; /text() extracts text content.</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(title)  <span class="co"># Output: My Title</span></span></code></pre></div>
<p>XPath allows for complex selections using various axes, predicates, and functions. Consult XPath documentation for detailed information.</p>
<h3 id="css-selectors">CSS Selectors</h3>
<p>CSS selectors provide a more concise and often more intuitive way to select elements compared to XPath, especially for developers familiar with CSS. Scrapy’s CSS selectors utilize the same syntax as CSS used in web styling.</p>
<p><strong>Example:</strong> Using the same HTML snippet above:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scrapy.selector <span class="im">import</span> Selector</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>html_content <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;html&gt;</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;body&gt;</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="st">  &lt;h1&gt;My Title&lt;/h1&gt;</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="st">  &lt;p&gt;This is some text.&lt;/p&gt;</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="st">  &lt;div&gt;</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="st">    &lt;p class="price"&gt;$100&lt;/p&gt;</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="st">  &lt;/div&gt;</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/body&gt;</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/html&gt;</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>selector <span class="op">=</span> Selector(text<span class="op">=</span>html_content)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>price <span class="op">=</span> selector.css(<span class="st">"p.price::text"</span>).get() <span class="co"># Selects p elements with class "price" and extracts text.</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(price) <span class="co"># Output: $100</span></span></code></pre></div>
<p>CSS selectors are generally easier to read and write for simpler selections but might lack the expressive power of XPath for very complex scenarios.</p>
<h3 id="extracting-data-with-selectors">Extracting Data with Selectors</h3>
<p>Both XPath and CSS selectors return <code>SelectorList</code> objects. The <code>get()</code> method extracts the first matching element’s value. The <code>getall()</code> method extracts values from all matching elements as a list. The <code>extract()</code> method is an alias for <code>getall()</code>.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scrapy.selector <span class="im">import</span> Selector</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>html_content <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;html&gt;</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;body&gt;</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="st">  &lt;p&gt;Item 1&lt;/p&gt;</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="st">  &lt;p&gt;Item 2&lt;/p&gt;</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="st">  &lt;p&gt;Item 3&lt;/p&gt;</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/body&gt;</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/html&gt;</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>selector <span class="op">=</span> Selector(text<span class="op">=</span>html_content)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>first_item <span class="op">=</span> selector.css(<span class="st">"p::text"</span>).get()     <span class="co">#Gets only the first item</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>all_items <span class="op">=</span> selector.css(<span class="st">"p::text"</span>).getall()  <span class="co">#Gets all items as a list</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(first_item) <span class="co"># Output: Item 1</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(all_items) <span class="co"># Output: ['Item 1', 'Item 2', 'Item 3']</span></span></code></pre></div>
<h3 id="selector-lists-and-iteration">Selector Lists and Iteration</h3>
<p>When multiple elements match a selector, a <code>SelectorList</code> object is returned. You can iterate over this list to process each selected element individually.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scrapy.selector <span class="im">import</span> Selector</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>html_content <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;html&gt;</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;body&gt;</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="st">  &lt;p&gt;Item 1&lt;/p&gt;</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="st">  &lt;p&gt;Item 2&lt;/p&gt;</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="st">  &lt;p&gt;Item 3&lt;/p&gt;</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/body&gt;</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="st">&lt;/html&gt;</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>selector <span class="op">=</span> Selector(text<span class="op">=</span>html_content)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> item <span class="kw">in</span> selector.css(<span class="st">"p::text"</span>):</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(item.get()) <span class="co">#Output: Item 1, Item 2, Item 3, each on a new line.</span></span></code></pre></div>
<p>This allows for flexible and powerful data extraction even when dealing with multiple occurrences of the target elements within the HTML or XML response. Remember to handle potential exceptions (e.g., <code>IndexError</code>) when accessing elements from a <code>SelectorList</code> if you are unsure of the number of matching items.</p>
<h2 id="spiders">Spiders</h2>
<p>Spiders are the core components of Scrapy that crawl websites and extract data. They define how Scrapy navigates through a website, follows links, and processes the extracted data.</p>
<h3 id="creating-a-spider">Creating a Spider</h3>
<p>To create a spider, you define a Python class that inherits from <code>scrapy.Spider</code>. This class needs to specify at least the <code>name</code> attribute and a method to define how the spider should start scraping, typically the <code>start_requests()</code> method.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scrapy</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MySpider(scrapy.Spider):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    name <span class="op">=</span> <span class="st">"my_spider"</span> <span class="co">#Unique spider identifier</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> start_requests(<span class="va">self</span>):</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        urls <span class="op">=</span> [<span class="st">"http://www.example.com"</span>, <span class="st">"http://www.example.org"</span>]</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> url <span class="kw">in</span> urls:</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">yield</span> scrapy.Request(url<span class="op">=</span>url, callback<span class="op">=</span><span class="va">self</span>.parse)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> parse(<span class="va">self</span>, response):</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This method will be called for each response received.</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract data from the response using selectors</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span></code></pre></div>
<p>This simple spider defines <code>start_requests()</code> to generate initial requests to two URLs and assigns the <code>parse()</code> method to handle the responses.</p>
<h3 id="spider-attributes">Spider Attributes</h3>
<p>Spiders have several important attributes:</p>
<ul>
<li><p><strong><code>name</code> (required):</strong> A unique identifier for your spider. This is how you’ll run your spider from the command line (e.g., <code>scrapy crawl my_spider</code>).</p></li>
<li><p><strong><code>start_urls</code> (optional):</strong> A list of URLs where the spider should begin crawling. If you use <code>start_urls</code>, you typically don’t need <code>start_requests()</code>.</p></li>
<li><p><strong><code>allowed_domains</code> (optional):</strong> A list of allowed domains to crawl. This helps prevent your spider from accidentally crawling unrelated sites.</p></li>
<li><p><strong><code>custom_settings</code> (optional):</strong> A dictionary of custom settings for this specific spider, overriding global Scrapy settings.</p></li>
</ul>
<h3 id="requesting-urls">Requesting URLs</h3>
<p>The <code>scrapy.Request</code> object is used to make requests to URLs. Key arguments include:</p>
<ul>
<li><strong><code>url</code> (required):</strong> The URL to request.</li>
<li><strong><code>callback</code> (required):</strong> The method to be called when the response is received.</li>
<li><strong><code>method</code> (optional):</strong> HTTP method (‘GET’, ‘POST’, etc.).</li>
<li><strong><code>headers</code> (optional):</strong> HTTP headers to include in the request.</li>
<li><strong><code>body</code> (optional):</strong> Request body (for POST requests).</li>
<li><strong><code>meta</code> (optional):</strong> A dictionary of metadata to pass along with the request. Useful for passing data between callbacks.</li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="cf">yield</span> scrapy.Request(url<span class="op">=</span><span class="st">"http://example.com/page2"</span>, callback<span class="op">=</span><span class="va">self</span>.parse_page2, meta<span class="op">=</span>{<span class="st">'page'</span>: <span class="dv">2</span>})</span></code></pre></div>
<h3 id="parsing-responses">Parsing Responses</h3>
<p>The <code>callback</code> method (e.g., <code>parse</code>, <code>parse_page2</code>) receives a <code>scrapy.http.Response</code> object as an argument. This object contains the HTTP response (status code, headers, body) and allows you to access the response content using selectors (as discussed previously).</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> parse_page2(<span class="va">self</span>, response):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    page <span class="op">=</span> response.meta[<span class="st">'page'</span>] <span class="co">#Access metadata passed via meta</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    title <span class="op">=</span> response.css(<span class="st">'title::text'</span>).get()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Page </span><span class="sc">{</span>page<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>title<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<h3 id="handling-different-response-types">Handling Different Response Types</h3>
<p>Scrapy handles various response types (HTML, JSON, XML, etc.). You’ll use appropriate selectors (XPath or CSS for HTML/XML, JSONPath for JSON) to extract data depending on the response content type. For JSON, you can use <code>response.json()</code> to parse the JSON response into a Python dictionary or list.</p>
<h3 id="spider-middleware">Spider Middleware</h3>
<p>Spider middleware are hooks that can be inserted into Scrapy’s spider execution pipeline. They allow you to modify requests and responses before they are processed by the spider. Common use cases include:</p>
<ul>
<li><strong>Request modification:</strong> Adding headers, modifying URLs.</li>
<li><strong>Response modification:</strong> Cleaning up HTML, handling errors.</li>
</ul>
<h3 id="rules-and-crawlspider">Rules and CrawlSpider</h3>
<p>For spiders that need to crawl websites more systematically by following links, <code>scrapy.linkextractors.LinkExtractor</code> and <code>scrapy.spiders.CrawlSpider</code> provide a more convenient approach. <code>LinkExtractor</code> defines how to extract links from responses, and <code>CrawlSpider</code> uses <code>rules</code> to define how to follow those links and what callback methods to use.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scrapy.linkextractors <span class="im">import</span> LinkExtractor</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scrapy.spiders <span class="im">import</span> CrawlSpider, Rule</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyCrawlSpider(CrawlSpider):</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    name <span class="op">=</span> <span class="st">"my_crawl_spider"</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    start_urls <span class="op">=</span> [<span class="st">"http://www.example.com"</span>]</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    rules <span class="op">=</span> (</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        Rule(LinkExtractor(allow<span class="op">=</span><span class="vs">r"category/</span><span class="dv">\d</span><span class="op">+</span><span class="vs">"</span>), callback<span class="op">=</span><span class="st">"parse_item"</span>, follow<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> parse_item(<span class="va">self</span>, response):</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Process items from the extracted links</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>      <span class="cf">pass</span></span></code></pre></div>
<p>This <code>CrawlSpider</code> uses <code>LinkExtractor</code> to find links matching <code>category/\d+</code> and calls <code>parse_item</code> for each matched link, recursively following links within the same category. <code>follow=True</code> makes the spider follow links matching the rule.</p>
<h2 id="items">Items</h2>
<p>Items are the fundamental data structures used in Scrapy to store the data extracted from websites. They provide a structured and consistent way to handle scraped data, making it easier to process and export.</p>
<h3 id="defining-items">Defining Items</h3>
<p>Items are defined using the <code>scrapy.Item</code> class. You define fields within the item using class attributes, each with a corresponding data type. These fields represent the specific data points you want to extract.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scrapy</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ProductItem(scrapy.Item):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    name <span class="op">=</span> scrapy.Field()</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    price <span class="op">=</span> scrapy.Field()</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    description <span class="op">=</span> scrapy.Field()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    url <span class="op">=</span> scrapy.Field()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    image_urls <span class="op">=</span> scrapy.Field()</span></code></pre></div>
<p>This <code>ProductItem</code> defines fields for product name, price, description, URL, and a list of image URLs. The <code>scrapy.Field()</code> represents a field capable of holding various data types.</p>
<h3 id="working-with-item-loaders">Working with Item Loaders</h3>
<p>Item Loaders provide a convenient and efficient way to populate <code>Item</code> objects with data extracted from web pages. They handle the process of mapping data extracted from selectors to the appropriate item fields, including data cleaning and validation.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scrapy</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scrapy.loader <span class="im">import</span> ItemLoader</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> .items <span class="im">import</span> ProductItem</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ProductSpider(scrapy.Spider):</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    name <span class="op">=</span> <span class="st">"product_spider"</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... other attributes ...</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> parse(<span class="va">self</span>, response):</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        loader <span class="op">=</span> ItemLoader(item<span class="op">=</span>ProductItem(), response<span class="op">=</span>response)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        loader.add_css(<span class="st">"name"</span>, <span class="st">"h1.product-title::text"</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        loader.add_css(<span class="st">"price"</span>, <span class="st">"span.price::text"</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        loader.add_css(<span class="st">"description"</span>, <span class="st">"div.product-description p::text"</span>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        loader.add_value(<span class="st">"url"</span>, response.url)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        loader.add_css(<span class="st">"image_urls"</span>, <span class="st">"img.product-image::attr(src)"</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> loader.load_item()</span></code></pre></div>
<p>Here, an <code>ItemLoader</code> is used to populate the <code>ProductItem</code>. The <code>add_css()</code> method extracts data using CSS selectors and populates the corresponding fields in the <code>Item</code>. <code>add_value()</code> directly adds a value to the item. <code>load_item()</code> returns a fully populated <code>ProductItem</code> instance.</p>
<h3 id="item-pipelines">Item Pipelines</h3>
<p>Item pipelines are components that process items after they have been scraped. They allow you to perform actions like:</p>
<ul>
<li><strong>Data cleaning:</strong> Removing unwanted characters, formatting data.</li>
<li><strong>Data validation:</strong> Checking for missing or invalid data.</li>
<li><strong>Data persistence:</strong> Saving items to databases, files, or other storage systems.</li>
</ul>
<p>Pipelines are defined as Python classes containing methods that process items. The most important method is <code>process_item()</code>, which is called for each item.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scrapy</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyPipeline(<span class="bu">object</span>):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> process_item(<span class="va">self</span>, item, spider):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Perform data cleaning or validation</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'price'</span> <span class="kw">in</span> item:</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>            item[<span class="st">'price'</span>] <span class="op">=</span> <span class="bu">float</span>(item[<span class="st">'price'</span>].replace(<span class="st">'$'</span>, <span class="st">''</span>).replace(<span class="st">','</span>, <span class="st">''</span>))</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save item to a file (example)</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(<span class="st">"output.csv"</span>, <span class="st">"a"</span>) <span class="im">as</span> f:</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>            f.write(<span class="ss">f"</span><span class="sc">{</span>item[<span class="st">'name'</span>]<span class="sc">}</span><span class="ss">,</span><span class="sc">{</span>item[<span class="st">'price'</span>]<span class="sc">}</span><span class="ss">,</span><span class="sc">{</span>item[<span class="st">'url'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> item</span></code></pre></div>
<p>In this example, the pipeline removes ‘$’ and ‘,’ from the price field, converts it to a float, and saves the item to a CSV file. Multiple pipelines can be enabled in the <code>settings.py</code> to perform a series of actions. The <code>process_item()</code> method should always return the processed item. The pipeline should be specified in <code>settings.py</code> under <code>ITEM_PIPELINES</code>.</p>
<h2 id="item-pipelines-1">Item Pipelines</h2>
<p>Item pipelines are sequential components in Scrapy that process items after they have been scraped by spiders. They provide a mechanism to perform various operations on items before they are stored or further processed.</p>
<h3 id="pipeline-structure">Pipeline Structure</h3>
<p>A pipeline is a Python class that implements several methods, the most important of which is <code>process_item()</code>. This method receives an item and a spider as arguments and is responsible for processing the item. Other optional methods like <code>open_spider()</code>, <code>close_spider()</code>, <code>process_item()</code>, and <code>open_spider()</code> allow for setup and teardown actions related to the spider’s lifecycle.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyPipeline(<span class="bu">object</span>):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> open_spider(<span class="va">self</span>, spider):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Perform actions when a spider opens (e.g., open a database connection)</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> process_item(<span class="va">self</span>, item, spider):</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Process the item</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> item  <span class="co"># Always return the item</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> close_spider(<span class="va">self</span>, spider):</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Perform actions when a spider closes (e.g., close a database connection)</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span></code></pre></div>
<p>The <code>process_item()</code> method should always return an <code>Item</code> or raise a <code>DropItem</code> exception to drop the item from the pipeline.</p>
<h3 id="common-pipeline-operations">Common Pipeline Operations</h3>
<p>Common pipeline operations include:</p>
<ul>
<li><strong>Data Cleaning:</strong> Removing extra whitespace, converting data types, handling invalid characters.</li>
<li><strong>Data Validation:</strong> Checking for missing or invalid data, ensuring data consistency.</li>
<li><strong>Data Transformation:</strong> Modifying data based on certain conditions or rules. For example, converting currency formats, or transforming dates.</li>
<li><strong>Data Deduplication:</strong> Removing duplicate items.</li>
<li><strong>Persistence:</strong> Storing items in a database (e.g., MongoDB, SQL databases), a file (e.g., JSON, CSV), or other storage systems.</li>
</ul>
<h3 id="custom-pipelines">Custom Pipelines</h3>
<p>To create a custom pipeline, define a Python class that inherits (or does not inherit, depending on how you prefer to work with the framework) from <code>object</code> and implement the necessary methods, specifically <code>process_item()</code>. Then, enable it in your project’s <code>settings.py</code> file by adding it to the <code>ITEM_PIPELINES</code> setting. The key is an integer representing the order of execution, and the value is the fully qualified path to your pipeline class.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># myproject/pipelines.py</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> JsonWriterPipeline:</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> open_spider(<span class="va">self</span>, spider):</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">file</span> <span class="op">=</span> <span class="bu">open</span>(<span class="st">'items.jl'</span>, <span class="st">'w'</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> close_spider(<span class="va">self</span>, spider):</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">file</span>.close()</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> process_item(<span class="va">self</span>, item, spider):</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>        line <span class="op">=</span> json.dumps(<span class="bu">dict</span>(item)) <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">file</span>.write(line)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> item</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co"># settings.py</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>ITEM_PIPELINES <span class="op">=</span> {</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'myproject.pipelines.JsonWriterPipeline'</span>: <span class="dv">300</span>,</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>This example shows a custom pipeline that writes items to a JSON file.</p>
<h3 id="pipeline-ordering">Pipeline Ordering</h3>
<p>The order in which pipelines process items is determined by the integer keys in the <code>ITEM_PIPELINES</code> setting in <code>settings.py</code>. Lower numbers indicate earlier processing. This allows you to chain pipelines together to perform operations sequentially. For instance, you might have a cleaning pipeline followed by a validation pipeline, and finally a persistence pipeline. Make sure there are no duplicate keys in the <code>ITEM_PIPELINES</code> settings, as this will lead to unexpected errors.</p>
<h2 id="data-storage">Data Storage</h2>
<p>Once Scrapy has collected data using spiders and processed it with pipelines, you need to store it persistently. This section covers common methods for storing Scrapy data.</p>
<h3 id="storing-data-in-files">Storing Data in Files</h3>
<p>Storing data in files is a straightforward approach, suitable for smaller datasets or when you need a simple, readily accessible format. Common file formats include JSON, CSV, and text files. You can achieve this using custom pipelines or external scripts.</p>
<p><strong>JSON:</strong> JSON is a human-readable format suitable for structured data. You can write items to a JSON file line by line using a custom pipeline:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scrapy</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> JsonWriterPipeline:</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> open_spider(<span class="va">self</span>, spider):</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">file</span> <span class="op">=</span> <span class="bu">open</span>(<span class="st">'items.json'</span>, <span class="st">'w'</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> close_spider(<span class="va">self</span>, spider):</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">file</span>.close()</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> process_item(<span class="va">self</span>, item, spider):</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        line <span class="op">=</span> json.dumps(<span class="bu">dict</span>(item)) <span class="op">+</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">file</span>.write(line)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> item</span></code></pre></div>
<p><strong>CSV:</strong> CSV (Comma Separated Values) is a simple text format for tabular data. Similar pipelines can be created using the <code>csv</code> module.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> csv</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scrapy</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CsvWriterPipeline:</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> open_spider(<span class="va">self</span>, spider):</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">file</span> <span class="op">=</span> <span class="bu">open</span>(<span class="st">'items.csv'</span>, <span class="st">'w'</span>, newline<span class="op">=</span><span class="st">''</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.writer <span class="op">=</span> csv.writer(<span class="va">self</span>.<span class="bu">file</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.writer.writerow(<span class="bu">list</span>(spider.item_fields.keys())) <span class="co">#Write header row</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> close_spider(<span class="va">self</span>, spider):</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">file</span>.close()</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> process_item(<span class="va">self</span>, item, spider):</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.writer.writerow(item.values())</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> item</span></code></pre></div>
<p><strong>Text Files:</strong> For simpler data, you can write to plain text files.</p>
<h3 id="storing-data-in-databases">Storing Data in Databases</h3>
<p>For larger datasets or more complex data relationships, using a database is recommended. Scrapy supports various databases through custom pipelines. Common choices include:</p>
<ul>
<li><p><strong>SQL Databases (PostgreSQL, MySQL, SQLite):</strong> These relational databases are suitable for structured data with clear relationships between different data points. You’ll need database drivers (e.g., <code>psycopg2</code> for PostgreSQL) and create custom pipelines to interact with the database.</p></li>
<li><p><strong>NoSQL Databases (MongoDB):</strong> NoSQL databases are more flexible and can handle unstructured or semi-structured data. The <code>pymongo</code> driver is commonly used to interact with MongoDB from within a Scrapy pipeline.</p></li>
</ul>
<p>Example using <code>SQLAlchemy</code> to write to a SQLite database (requires installing <code>SQLAlchemy</code>):</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sqlalchemy</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sqlalchemy <span class="im">import</span> create_engine, text</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sqlalchemy.orm <span class="im">import</span> sessionmaker</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SqlitePipeline:</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> open_spider(<span class="va">self</span>, spider):</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.engine <span class="op">=</span> create_engine(<span class="st">'sqlite:///mydatabase.db'</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Session <span class="op">=</span> sessionmaker(bind<span class="op">=</span><span class="va">self</span>.engine)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="va">self</span>.Session() <span class="im">as</span> session:</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>            <span class="co">#Create table if it doesn't exist (adjust to your schema)</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>            session.execute(text(<span class="st">"""CREATE TABLE IF NOT EXISTS products (id INTEGER PRIMARY KEY, name TEXT, price REAL)"""</span>))</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>            session.commit()</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> process_item(<span class="va">self</span>, item, spider):</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="va">self</span>.Session() <span class="im">as</span> session:</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>            session.execute(text(<span class="st">"""INSERT INTO products (name, price) VALUES (:name, :price)"""</span>), {<span class="st">"name"</span>: item[<span class="st">'name'</span>], <span class="st">"price"</span>: item[<span class="st">'price'</span>]})</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>            session.commit()</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> item</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<h3 id="exporting-data-in-various-formats">Exporting Data in Various Formats</h3>
<p>After storing the data, you might want to export it in various formats for analysis, visualization, or further processing. This can be done using custom scripts or libraries.</p>
<ul>
<li><strong>JSON:</strong> Python’s <code>json</code> module can export data to JSON files.</li>
<li><strong>CSV:</strong> Python’s <code>csv</code> module provides functions for exporting to CSV files.</li>
<li><strong>XML:</strong> Libraries like <code>xml.etree.ElementTree</code> or <code>lxml</code> can generate XML data.</li>
<li><strong>Parquet:</strong> The <code>pyarrow</code> or <code>fastparquet</code> libraries enable efficient storage and retrieval of data in the Parquet columnar storage format, which is highly suitable for large datasets.</li>
<li><strong>Other formats:</strong> Numerous other libraries provide support for different formats, such as XLSX (for Excel files), HDF5 (for large, complex datasets), etc. The best choice depends on your specific requirements.</li>
</ul>
<p>Remember that for large datasets, handling data efficiently during export is crucial. Using appropriate libraries and techniques to minimize memory usage and maximize performance is essential.</p>
<h2 id="request-and-response-handling">Request and Response Handling</h2>
<p>Efficiently managing requests and responses is crucial for effective web scraping. Scrapy provides tools to make requests, handle responses, and manage HTTP headers and cookies.</p>
<h3 id="making-requests">Making Requests</h3>
<p>Scrapy uses the <code>Request</code> object to make HTTP requests to websites. The <code>Request</code> object takes several parameters, allowing you to customize your requests.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scrapy</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MySpider(scrapy.Spider):</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    name <span class="op">=</span> <span class="st">"my_spider"</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    start_urls <span class="op">=</span> [<span class="st">"http://www.example.com"</span>]</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> start_requests(<span class="va">self</span>):</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> url <span class="kw">in</span> <span class="va">self</span>.start_urls:</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">yield</span> scrapy.Request(url, callback<span class="op">=</span><span class="va">self</span>.parse)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> parse(<span class="va">self</span>, response):</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">#Process the response here</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span></code></pre></div>
<p>This simple example shows how to create a <code>Request</code> in the <code>start_requests</code> method of a spider. The <code>callback</code> argument specifies the method to call when the response is received. You can also specify <code>method</code> (GET, POST, etc.), <code>headers</code>, <code>cookies</code>, <code>body</code> (for POST requests), and <code>meta</code> (for passing data between callbacks) in the <code>Request</code> constructor. For POST requests, you’d include the <code>method='POST'</code> and <code>body</code> parameters.</p>
<h3 id="handling-responses">Handling Responses</h3>
<p>The <code>callback</code> method receives a <code>Response</code> object, which contains the HTTP response:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> parse(<span class="va">self</span>, response):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> response.status <span class="op">==</span> <span class="dv">200</span>:</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Process successful response</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> response.css(<span class="st">'title::text'</span>).get()</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Page title: </span><span class="sc">{</span>title<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Handle errors (e.g., 404, 500)</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"HTTP error: </span><span class="sc">{</span>response<span class="sc">.</span>status<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<p>The <code>response</code> object provides access to the HTTP status code (<code>response.status</code>), headers (<code>response.headers</code>), body (<code>response.body</code>), cookies (<code>response.cookies</code>), and allows data extraction using selectors. Always check the status code to handle potential errors.</p>
<h3 id="http-headers">HTTP Headers</h3>
<p>HTTP headers provide additional information about the request or response. You can set custom headers in the <code>Request</code> object to mimic browser behavior or provide authentication information.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>headers <span class="op">=</span> {</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'User-Agent'</span>: <span class="st">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="cf">yield</span> scrapy.Request(url, callback<span class="op">=</span><span class="va">self</span>.parse, headers<span class="op">=</span>headers)</span></code></pre></div>
<p>This example adds a <code>User-Agent</code> header to the request. This is often crucial to avoid being blocked by websites that detect bot-like requests.</p>
<h3 id="cookies">Cookies</h3>
<p>Cookies are small pieces of data stored by websites to maintain state across multiple requests. You can manage cookies in Scrapy by setting them in <code>Request</code> objects or extracting them from responses.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>cookies <span class="op">=</span> {<span class="st">'sessionid'</span>: <span class="st">'your_session_id'</span>}</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="cf">yield</span> scrapy.Request(url, callback<span class="op">=</span><span class="va">self</span>.parse, cookies<span class="op">=</span>cookies)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Access cookies from a response:</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cookie <span class="kw">in</span> response.cookies:</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Cookie: </span><span class="sc">{</span>cookie<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<h3 id="request-and-response-middleware">Request and Response Middleware</h3>
<p>Request and response middleware are components that intercept and modify requests and responses before they reach the spider or after they are processed by the spider. They allow for centralized handling of tasks like:</p>
<ul>
<li><strong>Request modification:</strong> Adding headers, changing URLs, retrying failed requests.</li>
<li><strong>Response modification:</strong> Cleaning up HTML, handling errors, converting data formats.</li>
<li><strong>Robots.txt compliance:</strong> Checking <code>robots.txt</code> rules before making requests.</li>
<li><strong>Proxy usage:</strong> Routing requests through proxies.</li>
</ul>
<p>Middleware is defined as a class inheriting from <code>ProcessRequest</code> (for request middleware) or <code>ProcessResponse</code> (for response middleware). These classes define <code>process_request</code> and <code>process_response</code> methods. Register middleware in the <code>DOWNLOADER_MIDDLEWARES</code> or <code>SPIDER_MIDDLEWARES</code> settings in <code>settings.py</code>. This allows you to apply modifications or checks to requests and responses across all or a specific group of your spiders.</p>
<h2 id="middleware">Middleware</h2>
<p>Middleware components in Scrapy provide a way to intercept and modify requests and responses. This allows for centralized handling of cross-cutting concerns without modifying individual spiders or pipelines. There are two main types: downloader middleware and spider middleware.</p>
<h3 id="downloader-middleware">Downloader Middleware</h3>
<p>Downloader middleware intercepts requests and responses as they pass between the Scrapy engine and the downloader. This is useful for tasks affecting the download process itself, such as:</p>
<ul>
<li><p><strong>Modifying requests:</strong> Adding or changing HTTP headers, cookies, or other request attributes. This is often used to emulate browser behavior more closely (e.g., adding a <code>User-Agent</code> header), to handle authentication, or to route requests through proxies.</p></li>
<li><p><strong>Handling responses:</strong> Processing responses before they reach the spider. This can involve error handling, cleaning up HTML, or converting response data.</p></li>
<li><p><strong>Robots.txt compliance:</strong> Checking the <code>robots.txt</code> file of a website to ensure you are respecting its rules before making requests.</p></li>
<li><p><strong>Retry mechanisms:</strong> Implementing retry logic for failed requests due to network issues or temporary errors.</p></li>
</ul>
<p>Downloader middleware classes implement <code>process_request()</code> and <code>process_response()</code> methods (and optionally <code>process_exception()</code>). <code>process_request()</code> is called before the request is sent, and can return a modified <code>Request</code> object, a different <code>Request</code> object, or <code>None</code> (to drop the request). <code>process_response()</code> is called after a response is received. It can return a modified <code>Response</code> object, or raise a <code>DropItem</code> exception to discard the response.</p>
<p>To use downloader middleware, register it in the <code>DOWNLOADER_MIDDLEWARES</code> setting in your <code>settings.py</code> file. The key is an integer indicating the order of execution (lower numbers are processed earlier), and the value is the full path to your middleware class.</p>
<h3 id="spider-middleware-1">Spider Middleware</h3>
<p>Spider middleware intercepts requests and responses as they pass between the Scrapy engine and spiders. This is suitable for tasks related to spider behavior and data processing:</p>
<ul>
<li><p><strong>Request modification:</strong> Modifying the requests generated by the spider, such as adding metadata or altering URLs.</p></li>
<li><p><strong>Response modification:</strong> Processing responses before they reach the spider’s <code>parse()</code> method. This could involve data cleaning, pre-processing, or filtering.</p></li>
<li><p><strong>Spider selection:</strong> Dynamically selecting which spiders to use based on some criteria (although this might be better handled in other ways in most cases).</p></li>
</ul>
<p>Spider middleware classes implement <code>process_spider_input()</code> and <code>process_spider_output()</code> methods (and optionally <code>process_start_requests()</code> and <code>process_spider_exception()</code>).</p>
<p>To use spider middleware, register it in the <code>SPIDER_MIDDLEWARES</code> setting in your <code>settings.py</code> file, similar to downloader middleware registration.</p>
<h3 id="creating-custom-middleware">Creating Custom Middleware</h3>
<p>To create custom middleware, create a class that implements the appropriate methods (<code>process_request</code>, <code>process_response</code>, <code>process_exception</code> for downloader middleware; <code>process_spider_input</code>, <code>process_spider_output</code>, <code>process_start_requests</code>, <code>process_spider_exception</code> for spider middleware). Then, register it in the relevant setting (<code>DOWNLOADER_MIDDLEWARES</code> or <code>SPIDER_MIDDLEWARES</code>) in your <code>settings.py</code> file.</p>
<p><strong>Example Downloader Middleware (adding a User-Agent header):</strong></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CustomUserAgentMiddleware:</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> process_request(<span class="va">self</span>, request, spider):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>        request.headers[<span class="st">'User-Agent'</span>] <span class="op">=</span> <span class="st">'My Custom User Agent'</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span> <span class="co"># or return request if you want to modify it further.</span></span></code></pre></div>
<p><strong>Example Spider Middleware (cleaning response data):</strong></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CleanResponseMiddleware:</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> process_spider_output(<span class="va">self</span>, response, result, spider):</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> item <span class="kw">in</span> result:</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(item, <span class="bu">dict</span>) <span class="kw">and</span> <span class="st">'text'</span> <span class="kw">in</span> item:</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>                item[<span class="st">'text'</span>] <span class="op">=</span> item[<span class="st">'text'</span>].strip() <span class="co">#Example clean-up</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>            <span class="cf">yield</span> item</span></code></pre></div>
<p>Remember to place your custom middleware classes in a relevant Python module within your Scrapy project and adjust the paths in your <code>settings.py</code> accordingly. The order of middleware execution is defined by the integer keys used during registration in <code>settings.py</code>. Properly choosing the order is crucial to ensure your middleware functions as intended.</p>
<h2 id="settings">Settings</h2>
<p>Scrapy’s behavior is controlled by settings, which can be configured in several ways, providing flexibility and control over various aspects of the framework.</p>
<h3 id="configuring-scrapy">Configuring Scrapy</h3>
<p>Scrapy settings determine how the framework behaves, affecting aspects such as concurrency, download delays, pipeline operations, and more. These settings are organized into a dictionary-like structure and are accessible throughout the Scrapy framework.</p>
<h3 id="setting-file">Setting File</h3>
<p>The primary way to configure Scrapy settings is through a <code>settings.py</code> file located in your project’s root directory. This file contains a Python dictionary where you can define and modify settings.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># settings.py</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>BOT_NAME <span class="op">=</span> <span class="st">'my_project'</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>SPIDER_MODULES <span class="op">=</span> [<span class="st">'my_project.spiders'</span>]</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>NEWSPIDER_MODULE <span class="op">=</span> <span class="st">'my_project.spiders'</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Example of setting concurrency</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>CONCURRENT_REQUESTS <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>DOWNLOAD_DELAY <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co">#Example of setting a custom pipeline</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>ITEM_PIPELINES <span class="op">=</span> {</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'my_project.pipelines.MyPipeline'</span>: <span class="dv">300</span>,</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="co">#Example of setting a custom downloader middleware</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>DOWNLOADER_MIDDLEWARES <span class="op">=</span> {</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">'my_project.middlewares.MyDownloaderMiddleware'</span>: <span class="dv">543</span>,</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>This example shows some common settings. Refer to Scrapy’s documentation for a complete list of available settings and their descriptions.</p>
<h3 id="command-line-options">Command-Line Options</h3>
<p>You can override settings using command-line options when running Scrapy commands. This is useful for temporary changes or for specific scenarios.</p>
<ul>
<li><p><code>-s SETTING=VALUE</code>: Sets a single setting. For example, <code>scrapy crawl myspider -s DOWNLOAD_DELAY=5</code> sets <code>DOWNLOAD_DELAY</code> to 5 for that specific crawl.</p></li>
<li><p><code>-a ARG=VALUE</code>: Passes a custom argument to your spider. This is not directly a setting but can influence how a spider behaves.</p></li>
<li><p><code>--set SETTING=VALUE</code>: Similar to <code>-s</code>, but allows setting multiple values using multiple <code>--set</code> flags.</p></li>
</ul>
<h3 id="environment-variables">Environment Variables</h3>
<p>Settings can also be overridden using environment variables. This approach is useful for managing settings across multiple environments (e.g., development, testing, production) without modifying the <code>settings.py</code> file directly. The environment variable name should be <code>SCRAPY_</code> followed by the setting name in uppercase, replacing dots with underscores. For example, to set <code>DOWNLOAD_DELAY</code>, you would use <code>export SCRAPY_DOWNLOAD_DELAY=10</code>.</p>
<p>Environment variables take precedence over the <code>settings.py</code> file and command-line options. This hierarchical structure allows for flexible configuration based on your needs: environment variables have the highest priority, followed by command-line options, and then the <code>settings.py</code> file. This enables easy adjustment of settings for various deployment environments and execution scenarios.</p>
<h2 id="crawling-strategies">Crawling Strategies</h2>
<p>Scrapy’s crawling behavior can be influenced by how you design your spiders and configure settings, leading to different crawling strategies. Two primary approaches are depth-first and breadth-first crawling.</p>
<h3 id="depth-first-crawling">Depth-First Crawling</h3>
<p>In depth-first crawling, the spider explores a branch of the website as deeply as possible before moving on to other branches. This is often achieved implicitly when following links recursively without explicit control over the order of requests. Consider a website structure like this:</p>
<pre><code>A
├── B
│   └── D
│       └── F
└── C
    └── E
        └── G</code></pre>
<p>A depth-first approach would crawl in the order A, B, D, F, C, E, G. This strategy might be suitable for websites where the most relevant information is located deep within the site’s hierarchy. However, it could also be inefficient if a lot of irrelevant pages are encountered deep down branches.</p>
<h3 id="breadth-first-crawling">Breadth-First Crawling</h3>
<p>In breadth-first crawling, the spider explores all links at a given depth before moving to the next level. This approach requires more explicit control over the order in which requests are processed. Using the same website structure example above, breadth-first crawling would visit nodes in the order A, B, C, D, E, F, G. This strategy is useful when you need to quickly cover a wide range of pages at a shallow depth. It’s also better suited to situations where you might want to find all pages at a certain level quickly, rather than exploring deeply into less relevant parts of the website.</p>
<h3 id="scheduling-requests">Scheduling Requests</h3>
<p>Scrapy’s scheduler manages the order in which requests are processed. By default, Scrapy uses a <code>FIFO</code> (First-In, First-Out) scheduler, which is essentially a queue. Requests are added to the queue and processed in the order they were added. This tends to lead to breadth-first crawling if you’re not explicitly controlling the link-following order. More sophisticated schedulers could prioritize certain requests, resulting in customized crawling behaviors. You can implement custom schedulers for more intricate crawling strategies, but the default is generally sufficient for most use-cases.</p>
<h3 id="prioritizing-requests">Prioritizing Requests</h3>
<p>While the default scheduler processes requests in a FIFO manner, you can influence the order of requests using priorities. You can assign priorities to individual requests using the <code>priority</code> attribute in the <code>scrapy.Request</code> object. Requests with higher priority values (numerically larger) are processed before requests with lower priority values. This allows you to prioritize specific parts of the website or certain types of pages. Note that this will still operate within the basic queueing mechanism of the default scheduler; it merely affects the position of the request within the queue.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="cf">yield</span> scrapy.Request(url, callback<span class="op">=</span><span class="va">self</span>.parse, priority<span class="op">=</span><span class="dv">10</span>) <span class="co">#High priority</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="cf">yield</span> scrapy.Request(other_url, callback<span class="op">=</span><span class="va">self</span>.parse_other) <span class="co">#Default priority (0)</span></span></code></pre></div>
<p>In this example, the request to <code>url</code> will be processed before the request to <code>other_url</code> because it has a higher priority. Combining priority settings with well-defined <code>start_urls</code> and careful link extraction strategies enables the creation of focused and efficient crawling behaviors for your projects. However, excessively complex prioritization schemes might negate the benefits of parallel processing and increase the complexity of your code.</p>
<h2 id="debugging-and-logging">Debugging and Logging</h2>
<p>Debugging and logging are essential for developing and maintaining robust Scrapy applications. This section describes techniques and tools for identifying and resolving issues in your Scrapy projects.</p>
<h3 id="debugging-scrapy-applications">Debugging Scrapy Applications</h3>
<p>Debugging Scrapy applications often involves inspecting requests, responses, and the flow of data through spiders and pipelines. Common approaches include:</p>
<ul>
<li><p><strong>Print statements:</strong> The simplest approach is to add <code>print()</code> statements at strategic points in your code to examine variables and the execution flow. While simple, this can become cumbersome for large projects and might not be suitable for production environments.</p></li>
<li><p><strong>Logging:</strong> Using Scrapy’s logging system provides a more structured and maintainable way to track events and debug issues. It allows for different logging levels (DEBUG, INFO, WARNING, ERROR, CRITICAL), making it easier to control the amount of information produced.</p></li>
<li><p><strong>Interactive debugging:</strong> Use a Python debugger (e.g., <code>pdb</code>) to step through your code, inspect variables, and examine the call stack. You can start the debugger in your code with <code>import pdb; pdb.set_trace()</code>.</p></li>
<li><p><strong>Inspecting responses:</strong> Examine the HTML or other content of responses using Scrapy’s shell (<code>scrapy shell &lt;URL&gt;</code>) or by printing the response content within your spider callbacks. This helps in verifying data extraction logic and identifying potential issues with selectors.</p></li>
<li><p><strong>Inspecting requests:</strong> Similarly, you can inspect requests made by your spiders to ensure they are correctly formed and contain the necessary headers, cookies, and data.</p></li>
</ul>
<h3 id="logging-in-scrapy">Logging in Scrapy</h3>
<p>Scrapy utilizes Python’s logging module. You can use the <code>logging</code> module directly in your code, or leverage Scrapy’s built-in logging configuration. Scrapy’s logging configuration is flexible and allows you to direct logs to different destinations (console, file) and at different log levels. By default, logs are written to the console. Adjusting the <code>LOG_LEVEL</code> setting in <code>settings.py</code> allows you to control the verbosity of your logging.</p>
<p>To add custom logging statements:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>logger <span class="op">=</span> logging.getLogger(<span class="va">__name__</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> my_function(response):</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    logger.debug(<span class="st">"Entering my_function with response: </span><span class="sc">%s</span><span class="st">"</span>, response)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... your code ...</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    logger.info(<span class="st">"Successfully processed response"</span>)</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> some_data</span></code></pre></div>
<h3 id="debugging-tools">Debugging Tools</h3>
<p>Beyond basic debugging techniques, several tools can enhance your debugging workflow:</p>
<ul>
<li><p><strong>Scrapy Shell:</strong> The Scrapy shell (<code>scrapy shell &lt;URL&gt;</code>) allows interactive exploration of web pages. You can test selectors, inspect response data, and experiment with different approaches to extract data without running your full spider.</p></li>
<li><p><strong>Scrapy Debug Mode:</strong> Running Scrapy in debug mode (<code>scrapy crawl myspider -d</code>) provides detailed information about requests, responses, and the spider’s execution flow. It’s particularly helpful for identifying bottlenecks or unexpected behavior.</p></li>
<li><p><strong>Remote Debugging:</strong> For more complex debugging tasks, remote debugging can be beneficial. Attach a debugger (such as <code>pdb</code> or a dedicated IDE debugger) to your running Scrapy process to step through the code, inspect variables, and analyze execution flow remotely.</p></li>
<li><p><strong>Profiling:</strong> For performance analysis, profiling tools can help identify performance bottlenecks in your code. This allows for optimizing your spider’s efficiency, especially for large-scale crawls. Tools like <code>cProfile</code> can provide detailed information about the execution time of different parts of your code.</p></li>
</ul>
<p>Effective debugging relies on a combination of these techniques. Use the simplest methods first (print statements, basic logging), but leverage more advanced tools (debugger, Scrapy shell, debug mode) as needed to tackle more complex issues and optimize your code’s efficiency and performance.</p>
<h2 id="testing">Testing</h2>
<p>Testing is crucial for ensuring the reliability and maintainability of your Scrapy projects. This section outlines strategies for testing different components of your Scrapy applications.</p>
<h3 id="unit-testing-scrapy-spiders">Unit Testing Scrapy Spiders</h3>
<p>Unit testing focuses on individual components in isolation. For spiders, this means testing the parsing logic without involving the actual crawling process. Use mocking to simulate responses and test how your spider processes them. The <code>unittest</code> module (or <code>pytest</code>) is commonly used for writing unit tests.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> unittest</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> unittest.mock <span class="im">import</span> Mock</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> myproject.spiders.example <span class="im">import</span> ExampleSpider  <span class="co"># Your spider</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TestExampleSpider(unittest.TestCase):</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> setUp(<span class="va">self</span>):</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.spider <span class="op">=</span> ExampleSpider()</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.response_mock <span class="op">=</span> Mock()</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_parse_page(<span class="va">self</span>):</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mock a response (replace with your actual HTML)</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.response_mock.css.return_value.getall.return_value <span class="op">=</span> [<span class="st">"Item 1"</span>, <span class="st">"Item 2"</span>]</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        items <span class="op">=</span> <span class="bu">list</span>(<span class="va">self</span>.spider.parse(<span class="va">self</span>.response_mock))</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(<span class="bu">len</span>(items), <span class="dv">2</span>)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(items[<span class="dv">0</span>][<span class="st">'name'</span>], <span class="st">"Item 1"</span>) <span class="co">#Assumes your spider extracts 'name' field</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(items[<span class="dv">1</span>][<span class="st">'name'</span>], <span class="st">"Item 2"</span>)</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">'__main__'</span>:</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>    unittest.main()</span></code></pre></div>
<p>This example uses <code>unittest.mock</code> to simulate a response. You would replace the mock response with sample HTML data representative of what your spider expects to receive. Test assertions verify that the spider correctly extracts and processes the data. Use a testing framework like <code>pytest</code> for more advanced features and a cleaner syntax.</p>
<h3 id="integration-testing">Integration Testing</h3>
<p>Integration tests verify the interaction between different components of your Scrapy application. These tests involve running a subset of your spider or the entire spider to check that data flows correctly from requests, through parsing, and into pipelines. You’ll likely need to use real or realistic mock HTTP responses for these tests. This helps identify issues in the interaction between spiders, pipelines, and middleware.</p>
<p>A simple approach might involve running your spider against a small, controlled subset of a website and asserting that the output matches your expectations. For larger sites, this might necessitate using a small, self-contained test environment or creating sophisticated mocks for external systems that your pipelines might interact with.</p>
<h3 id="testing-pipelines">Testing Pipelines</h3>
<p>Pipelines process items after they are scraped. Testing pipelines involves verifying that they perform the intended operations correctly: cleaning data, validating data, and storing data. Similar to spider unit testing, you can use mocking to simulate items and test the pipeline’s behavior without needing a running spider.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> unittest</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> myproject.pipelines <span class="im">import</span> MyPipeline <span class="co">#Your pipeline</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TestMyPipeline(unittest.TestCase):</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> setUp(<span class="va">self</span>):</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pipeline <span class="op">=</span> MyPipeline()</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> test_process_item(<span class="va">self</span>):</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        item <span class="op">=</span> {<span class="st">'name'</span>: <span class="st">'Test Item'</span>, <span class="st">'price'</span>: <span class="st">'10.99'</span>}</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        processed_item <span class="op">=</span> <span class="va">self</span>.pipeline.process_item(item, <span class="va">None</span>)  <span class="co">#Pass None for spider, as not required here</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.assertEqual(processed_item[<span class="st">'price'</span>], <span class="fl">10.99</span>) <span class="co">#Assumes pipeline converts string to float</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">'__main__'</span>:</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    unittest.main()</span></code></pre></div>
<p>This example tests a simple pipeline that converts a price string to a float. Remember to adapt the tests to your specific pipeline functionality, verifying data cleaning, validation, and storage as appropriate. Consider using mocking for database interactions to avoid dependencies on external systems during testing. Integration tests for pipelines could involve checking that the data is correctly stored in the chosen database or file system.</p>
<p>Remember to write comprehensive tests covering various scenarios and edge cases to ensure the reliability and correctness of your Scrapy projects. Using a dedicated testing framework improves test organization and maintainability. Employing both unit and integration testing is crucial for achieving high-quality, robust Scrapy applications.</p>
<h2 id="advanced-topics">Advanced Topics</h2>
<p>This section covers more advanced aspects of Scrapy development, addressing common challenges and providing strategies for handling complex scenarios.</p>
<h3 id="working-with-javascript">Working with JavaScript</h3>
<p>Many websites use JavaScript to render content dynamically. Scrapy, by default, only processes the initial HTML response. To handle JavaScript-rendered content, you need to use a headless browser like Selenium, Playwright, or Splash. These tools render the JavaScript and provide the fully rendered HTML for Scrapy to process.</p>
<p><strong>Using Splash:</strong> Splash is a lightweight headless browser specifically designed for web scraping. You need to install and run Splash separately, and then configure Scrapy to use it as a rendering middleware. This involves adding Splash to your <code>DOWNLOADER_MIDDLEWARES</code> settings and using the <code>splash</code> request meta key in your requests.</p>
<p><strong>Using Selenium or Playwright:</strong> Selenium and Playwright are more general-purpose browser automation tools. They require more setup but provide more control over the browser’s behavior. You’ll typically write custom middleware to interact with these tools and render JavaScript-generated content.</p>
<p>Regardless of the approach, integrating JavaScript rendering adds complexity. Consider the tradeoffs between the increased complexity and the need to handle dynamically loaded content. Often, carefully examining the network requests made by a browser (using your browser’s developer tools) can reveal if you might be able to avoid using a headless browser entirely by directly fetching the data via API calls.</p>
<h3 id="handling-authentication">Handling Authentication</h3>
<p>Websites often require authentication to access certain parts. Scrapy provides ways to handle various authentication methods:</p>
<ul>
<li><p><strong>Basic Authentication:</strong> Use the <code>http_user</code> and <code>http_pass</code> parameters in the <code>Request</code> object to provide username and password credentials for basic HTTP authentication.</p></li>
<li><p><strong>Session Cookies:</strong> If a site uses cookies for authentication, you’ll likely need to extract the authentication cookies from a login response and include them in subsequent requests. This requires analyzing the login process and how the site manages session cookies.</p></li>
<li><p><strong>Forms:</strong> For websites that use login forms, you would submit the login form data (usually via a POST request) to obtain the authentication token (often cookies). You’ll have to analyze the form’s structure and the POST request parameters to emulate the login process accurately.</p></li>
<li><p><strong>API Keys:</strong> Many APIs require API keys for authentication. Include your API key in the request headers or as a query parameter, depending on the API’s documentation.</p></li>
</ul>
<h3 id="proxies-and-ip-rotation">Proxies and IP Rotation</h3>
<p>Using proxies can help prevent IP blocking from websites. Scrapy supports using proxies through the <code>DOWNLOADER_MIDDLEWARES</code>. You might need a custom middleware to manage a pool of proxies and rotate them periodically. This often involves fetching proxies from a proxy provider, storing them, and cycling through them in the middleware to ensure that your requests originate from different IP addresses. Note that ethical concerns and compliance with the terms of service of websites and proxy providers are paramount when using proxies.</p>
<h3 id="distributed-crawling">Distributed Crawling</h3>
<p>For large-scale crawling, distributing the workload across multiple machines can improve efficiency. Scrapy provides mechanisms for distributed crawling using Scrapyd. Scrapyd is a service that manages and runs Scrapy projects. You can deploy your project to Scrapyd instances on several machines, enabling parallel crawling and significantly speeding up the overall process. Scrapyd also helps manage queueing and scheduling requests across different worker nodes.</p>
<h3 id="scrapy-extensions">Scrapy Extensions</h3>
<p>Scrapy extensions add functionality to Scrapy. They extend the framework’s capabilities without directly modifying core components. Some built-in extensions include those for monitoring, logging, and other functionalities. You can create custom extensions to address specific needs in your project. They can be loaded similar to middleware, specified in the <code>EXTENSIONS</code> setting within <code>settings.py</code>. Extensions are a powerful tool for adding custom features to your Scrapy setup without altering core files.</p>
<p>Remember that ethical considerations are crucial when implementing these advanced techniques. Always respect <code>robots.txt</code>, comply with the terms of service of websites, and avoid overloading target servers. Responsible web scraping is essential to ensure the longevity and usefulness of these powerful tools.</p>
<h2 id="deploying-scrapy-projects">Deploying Scrapy Projects</h2>
<p>Deploying Scrapy projects involves moving your project to a server and setting up mechanisms for running and monitoring your spiders. This section outlines the steps involved.</p>
<h3 id="deploying-to-a-server">Deploying to a Server</h3>
<p>Deploying a Scrapy project typically involves these steps:</p>
<ol type="1">
<li><p><strong>Choose a Server:</strong> Select a server that meets your needs in terms of resources (CPU, memory, storage), operating system compatibility, and cost. Cloud providers (AWS, Google Cloud, Azure) offer scalable and cost-effective options. A virtual private server (VPS) is another common choice.</p></li>
<li><p><strong>Set up the Environment:</strong> Install Python and necessary dependencies on the server. Use a virtual environment (<code>venv</code> or <code>conda</code>) to isolate your project’s dependencies. Ensure that Scrapy and any project-specific libraries are installed.</p></li>
<li><p><strong>Transfer Project Files:</strong> Copy your Scrapy project files to the server. Use secure methods like <code>scp</code> or <code>rsync</code> for transferring files securely.</p></li>
<li><p><strong>Configure Settings:</strong> Adjust your <code>settings.py</code> file to reflect the server environment. Pay particular attention to settings that impact resource usage, such as <code>CONCURRENT_REQUESTS</code>, <code>DOWNLOAD_DELAY</code>, and <code>RETRY_TIMES</code>. Make sure any database connections or file paths are appropriate for the server environment.</p></li>
<li><p><strong>Test Deployment:</strong> Run a small test crawl on the server to verify that everything is working correctly before scheduling regular crawls.</p></li>
</ol>
<h3 id="scheduling-crawls">Scheduling Crawls</h3>
<p>Once your project is deployed, you need a way to schedule regular crawls. Several options exist:</p>
<ul>
<li><p><strong>System’s Cron Job (Linux):</strong> Use the server’s cron utility to schedule commands that run your spiders. A cron entry might look like this: <code>0 0 * * * /usr/bin/scrapy crawl myspider</code>. This runs <code>myspider</code> every day at midnight.</p></li>
<li><p><strong>Task Schedulers:</strong> Use task schedulers like APScheduler or Celery to manage complex schedules and handle failures gracefully. These tools provide more advanced features like retry mechanisms and better error handling.</p></li>
<li><p><strong>Scrapyd:</strong> Scrapyd is a service designed to run Scrapy projects. You deploy your project to Scrapyd, and then use its API or web interface to schedule and manage crawls. Scrapyd provides robust tools for managing spiders and monitoring their execution, including features like logging and error handling.</p></li>
<li><p><strong>Other Schedulers:</strong> Cloud platforms often provide their own scheduling services (e.g., AWS Lambda, Google Cloud Functions, Azure Functions). These can be integrated with Scrapyd or used directly to schedule the execution of a command to run your Scrapy spider.</p></li>
</ul>
<p>Choose a scheduling mechanism that best fits your project’s complexity and your familiarity with these technologies.</p>
<h3 id="monitoring-performance">Monitoring Performance</h3>
<p>Monitoring your deployed Scrapy projects is essential to ensure they run smoothly and efficiently. Methods for monitoring include:</p>
<ul>
<li><p><strong>Logs:</strong> Regularly check your Scrapy logs to identify errors, warnings, and other important events. Configure logging to send logs to a centralized location (e.g., a logging server) for easier monitoring.</p></li>
<li><p><strong>Metrics:</strong> Use metrics to track key performance indicators (KPIs) such as crawl speed, number of requests per second, number of items processed, and error rates. Tools like Prometheus and Grafana can help collect, visualize, and analyze these metrics. You would need to implement custom logging or instrumentation within your spiders and pipelines to capture this data.</p></li>
<li><p><strong>Scrapyd Web UI:</strong> If you’re using Scrapyd, its web interface provides a dashboard to monitor running jobs and their status.</p></li>
<li><p><strong>Monitoring Tools:</strong> Integrate with system monitoring tools to track resource usage (CPU, memory, network) of the server hosting your Scrapy project. This can help identify potential performance bottlenecks.</p></li>
</ul>
<p>Regularly monitoring your project’s performance and proactively addressing issues ensures a smoothly running and efficient data collection process. Early detection of problems prevents potential data loss or system failures.</p>
<p>Remember to prioritize security when deploying and monitoring your Scrapy projects. Protect your server with appropriate firewalls and security measures, and use secure methods for transferring files and managing credentials.</p>


<footer>Copyright 2025 - Muthukrishnan</footer>
<script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3609399560636561" crossorigin="anonymous"></script>




</body></html>